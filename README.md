# CoreAI-LLM

CoreAI-LLM is a **self-hosted LLM** project that exposes an HTTP API for chat using **Node.js + Express** and integration with **Ollama**. The goal is to allow the execution of language models locally, without dependence on cloud services, prioritizing simplicity, control, and low resource consumption.

---

## Overview

The project consists of three main parts:

* **Backend (API)**: Express server responsible for receiving requests, managing conversation context, and forwarding prompts to Ollama.

* **Local LLM (Ollama)**: Inference engine responsible for executing the language model.

* **Web Frontend**: Simple interface for interacting with the model via a browser.

This architecture allows for easy expansion to multiple models, point-of-sale improvements, and integration with other systems.

---

## Technologies Used

* Node.js
* Expresso
* Ollama
* JavaScript (ES Modules)
* HTML/CSS/JavaScript (front-end)

---

## Requirements

Before starting, make sure you have the following components installed:

* Node.js (version 18 or higher)
* npm or fio
* Ollama installed and working

To check if Ollama is active:

```
ollama --version
```

---

## Supported Templates

The project is designed to work with local templates available in Ollama. Examples:

* lhama3.2:1b
* qwen2
* mistral

The template can be modified directly in the backend, facilitating testing and comparisons.

---

## Installation

Clone the repository:

```
git clone https://github.com/cvinicius369/CoreAI-LLM.git cd CoreAI-LLM
```

Install the dependencies:

```
npm install
```

---

##Configuration

In the `server.js` file, you can configure:

* Ollama URL
* LLM model used
* Server port

Example of a typical configuration:

* Ollama URL: `http://127.0.0.1:11434`
* Model: `llama3.2:1b`

---

## Execution

To start the server:

```
npm start
```

The server will be available From:

```
http://localhost:3000
```

---

## API Usage

### Chat Endpoint

**POST** `/chat`

Request Body:

```
{
"message": "Hello, who are you?"

}
```

Response:

```
{
"response": "Response generated by the model"
}
```

---

## Web Frontend

The frontend allows direct interaction with the model via a browser, using a local API.

Features:

* Sending messages
* Displaying model responses
* Direct communication with the backend

The frontend is statically served by Express itself.

---

## Implemented Best Practices

* Clear separation between frontend and backend
* Use of JSON for communication
* Simple and extensible structure
* Local and controlled execution of the LLM

---

## Security

This project was designed for local use. Exporting the API directly to the internet is not recommended without:

* Authentication
* Rate limiting
* Reverse proxy
* Monitoring

---

## Author

Developed by **Caio Vin√≠cius**.

## Project Image

<image src="./print1.png"></image>
